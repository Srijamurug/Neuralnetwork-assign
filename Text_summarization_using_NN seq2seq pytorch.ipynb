{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1c4lcy8t7O6XHpkd0E6c6R86cH_nlNOP_",
      "authorship_tag": "ABX9TyNM/6rk7SJg7/NdiHRUd4Hs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Srijamurug/Neuralnetwork-assign/blob/main/Text_summarization_using_NN%20seq2seq%20pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# seq2seq with pytorch "
      ],
      "metadata": {
        "id": "MBBHmqwUcaGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Importing the necessary modules"
      ],
      "metadata": {
        "id": "lD2B6qX4Je4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "fu2-ZPRUI_lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n"
      ],
      "metadata": {
        "id": "FfvbrSibO8bO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a= pd.read_csv(\"/content/drive/MyDrive/bbc-text.csv\")"
      ],
      "metadata": {
        "id": "d40SyyeoI8TE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "R6Yl9BfdJQOu",
        "outputId": "6b35c2e9-d6e4-4742-fe42-4eae6d33417f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        category                                               text\n",
              "0           tech  tv future in the hands of viewers with home th...\n",
              "1       business  worldcom boss  left books alone  former worldc...\n",
              "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
              "3          sport  yeading face newcastle in fa cup premiership s...\n",
              "4  entertainment  ocean s twelve raids box office ocean s twelve..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-238c2beb-f029-4f91-a1ca-2b005ef761b8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tech</td>\n",
              "      <td>tv future in the hands of viewers with home th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>business</td>\n",
              "      <td>worldcom boss  left books alone  former worldc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sport</td>\n",
              "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sport</td>\n",
              "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>entertainment</td>\n",
              "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-238c2beb-f029-4f91-a1ca-2b005ef761b8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-238c2beb-f029-4f91-a1ca-2b005ef761b8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-238c2beb-f029-4f91-a1ca-2b005ef761b8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a['text'][:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDj-3VlFJTf_",
        "outputId": "8cd01b5f-9351-47ca-9fc1-f7462f723aa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    tv future in the hands of viewers with home th...\n",
              "1    worldcom boss  left books alone  former worldc...\n",
              "2    tigers wary of farrell  gamble  leicester say ...\n",
              "3    yeading face newcastle in fa cup premiership s...\n",
              "4    ocean s twelve raids box office ocean s twelve...\n",
              "5    howard hits back at mongrel jibe michael howar...\n",
              "6    blair prepares to name poll date tony blair is...\n",
              "7    henman hopes ended in dubai third seed tim hen...\n",
              "8    wilkinson fit to face edinburgh england captai...\n",
              "9    last star wars  not for children  the sixth an...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Cleaning the Data"
      ],
      "metadata": {
        "id": "_ox4THH_JaRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove non-alphabetic characters (Data Cleaning)\n",
        "def text_strip(column):\n",
        "\n",
        "    for row in column:\n",
        "        row = re.sub(\"(\\\\t)\", \" \", str(row)).lower()\n",
        "        row = re.sub(\"(\\\\r)\", \" \", str(row)).lower()\n",
        "        row = re.sub(\"(\\\\n)\", \" \", str(row)).lower()\n",
        "\n",
        "        # Remove _ if it occurs more than one time consecutively\n",
        "        row = re.sub(\"(__+)\", \" \", str(row)).lower()\n",
        "\n",
        "        # Remove - if it occurs more than one time consecutively\n",
        "        row = re.sub(\"(--+)\", \" \", str(row)).lower()\n",
        "\n",
        "        # Remove ~ if it occurs more than one time consecutively\n",
        "        row = re.sub(\"(~~+)\", \" \", str(row)).lower()\n",
        "\n",
        "        # Remove + if it occurs more than one time consecutively\n",
        "        row = re.sub(\"(\\+\\++)\", \" \", str(row)).lower()\n",
        "\n",
        "        # Remove . if it occurs more than one time consecutively\n",
        "        row = re.sub(\"(\\.\\.+)\", \" \", str(row)).lower()\n",
        "\n",
        "        # Remove the characters - <>()|&©ø\"',;?~*!\n",
        "        row = re.sub(r\"[<>()|&©ø\\[\\]\\'\\\",;?~*!]\", \" \", str(row)).lower()\n",
        "\n",
        "        # Remove mailto:\n",
        "        row = re.sub(\"(mailto:)\", \" \", str(row)).lower()\n",
        "\n",
        "        # Remove \\x9* in text\n",
        "        row = re.sub(r\"(\\\\x9\\d)\", \" \", str(row)).lower()\n",
        "\n",
        "        # Replace INC nums to INC_NUM\n",
        "        row = re.sub(\"([iI][nN][cC]\\d+)\", \"INC_NUM\", str(row)).lower()\n",
        "\n",
        "        # Replace CM# and CHG# to CM_NUM\n",
        "        row = re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", \"CM_NUM\", str(row)).lower()\n",
        "\n",
        "        # Remove punctuations at the end of a word\n",
        "        row = re.sub(\"(\\.\\s+)\", \" \", str(row)).lower()\n",
        "        row = re.sub(\"(\\-\\s+)\", \" \", str(row)).lower()\n",
        "        row = re.sub(\"(\\:\\s+)\", \" \", str(row)).lower()\n",
        "\n",
        "        # Replace any url to only the domain name\n",
        "        try:\n",
        "            url = re.search(r\"((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)\", str(row))\n",
        "            repl_url = url.group(3)\n",
        "            row = re.sub(r\"((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)\", repl_url, str(row))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Remove multiple spaces\n",
        "        row = re.sub(\"(\\s+)\", \" \", str(row)).lower()\n",
        "\n",
        "        # Remove the single character hanging between any two spaces\n",
        "        row = re.sub(\"(\\s+.\\s+)\", \" \", str(row)).lower()\n",
        "\n",
        "        yield row"
      ],
      "metadata": {
        "id": "JrJaGXEeJXXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "\n",
        "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
      ],
      "metadata": {
        "id": "AtMgMB42UGBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Preprocess text data using nltk. **\n",
        "\n",
        "It helps convert text into numbers, which the model can then easily work with."
      ],
      "metadata": {
        "id": "iKuTJlKrswDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords #to  ignored by typical tokenizers.\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower() # lowercase\n",
        "    text = text.split() # convert have'nt -> have not\n",
        "    for i in range(len(text)):\n",
        "        word = text[i]\n",
        "        if word in contraction_mapping:\n",
        "            text[i] = contraction_mapping[word]\n",
        "    text = \" \".join(text)\n",
        "    text = text.split()\n",
        "    newtext = []\n",
        "    for word in text:\n",
        "        if word not in stop_words:\n",
        "            newtext.append(word)\n",
        "    text = \" \".join(newtext)\n",
        "    text = text.replace(\"'s\",'') # convert your's -> your\n",
        "    text = re.sub(r'','',text) # remove (words)\n",
        "    text = re.sub(r'[^a-zA-Z0-9. ]','',text) # remove punctuations\n",
        "    text = re.sub(r'\\.',' . ',text)\n",
        "    return text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2H1oCvBPL3w",
        "outputId": "3b66c109-1c10-44ea-fe62-e21350507ccb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a['category'] = a['category'].apply(lambda x:preprocess(x))\n",
        "a['text'] = a['text'].apply(lambda x:preprocess(x))\n",
        "print(a['category'][20],a['text'][20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIkouQgvRQaw",
        "outputId": "6cd25365-c25c-4a8f-8ded-0c6b9d4ede5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tech security warning fbi virus us federal bureau investigation warning computer virus spread via emails purport fbi .  emails show come fbi . gov address tell recipients accessed illegal websites .  messages warn internet use monitored fbi internet fraud complaint center .  attachment email contains virus fbi said .  message asks recipients click attachment answer questions internet use .  rather questionnaire attachment contains virus infects recipient computer according agency .  clear virus infected computer .  users warned never open attachment unsolicited emails people know .  recipients similar solicitations know fbi engage practice sending unsolicited emails public manner fbi said statement .  bureau investigating phoney emails .  agency earlier month shut fbi . gov accounts used communicate public security breach .  spokeswoman said two incidents appear unrelated . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = a['category']\n",
        "y = a['text']\n",
        "print(x[50],y[50],sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NKhMt4ERfCh",
        "outputId": "a873e2f7-1d6b-406a-a1ee-780b67696a1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sport\n",
            "lewsey puzzle disallowed try england josh lewsey claimed denied late try side six nations loss ireland .  wasps wing insisted grounded ball bundled line said referee jonathan kaplan made wrong decision .  positive touched ball line lewsey told bbc sport .  certainly turnover .  driven put ball ground .  whistle went let go ball .  lewsey added one irish players scooped back whistle surprise referee gave turnover .  far concerned incident mark cueto effort charlie hodgson crossfield kick led looked like good try two key elements game .  cueto also puzzled try disallowed kaplan .  think could offside without doubt behind ball said sale player .  move planned technique cuff .  rehearse time time again .  say robbed decisions go go you .  today went us tough international level . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "2jBC-ANvRjrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "     "
      ],
      "metadata": {
        "id": "4PCNPB1pRmhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def readLangs(text, summary, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "    \n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[text[i],summary[i]] for i in range(len(text))]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(summary)\n",
        "        output_lang = Lang(text)\n",
        "    else:\n",
        "        input_lang = Lang(text)\n",
        "        output_lang = Lang(summary)\n",
        "\n",
        "    return input_lang, output_lang, pairs\n",
        "     "
      ],
      "metadata": {
        "id": "rvWS_YQaRpUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs"
      ],
      "metadata": {
        "id": "mTZ06XbKRrBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_lang, output_lang, pairs = prepareData( x, y , False)\n",
        "print(random.choice(pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6RtNKZ7Rsw7",
        "outputId": "6bfff572-3440-4bea-f0ea-6f1e69beb38b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 2225 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "0                tech\n",
            "1            business\n",
            "2               sport\n",
            "3               sport\n",
            "4       entertainment\n",
            "            ...      \n",
            "2220         business\n",
            "2221         politics\n",
            "2222    entertainment\n",
            "2223         politics\n",
            "2224            sport\n",
            "Name: category, Length: 2225, dtype: object 7\n",
            "0       tv future hands viewers home theatre systems p...\n",
            "1       worldcom boss left books alone former worldcom...\n",
            "2       tigers wary farrell gamble leicester say rushe...\n",
            "3       yeading face newcastle fa cup premiership side...\n",
            "4       ocean twelve raids box office ocean twelve cri...\n",
            "                              ...                        \n",
            "2220    cars pull us retail figures us retail sales fe...\n",
            "2221    kilroy unveils immigration policy exchatshow h...\n",
            "2222    rem announce new glasgow concert us band rem a...\n",
            "2223    political squabbles snowball become commonplac...\n",
            "2224    souness delight euro progress boss graeme soun...\n",
            "Name: text, Length: 2225, dtype: object 32146\n",
            "['politics', 'teens know little politics teenagers questioned survey shown little interest politics  little knowledge .  quarter 1416 year olds knew labour government tories official opposition lib dems third party .  almost could identify tony blair one six knew michael howard one 10 recognised charles kennedy .  icm survey interviewed 110 pupils education watchdog ofsted .  nearly half pupils polled said important know political parties stand for .  4 questioned thought conservatives power  2 believed lib dems were .  survey also looked issues nationality .  found union flag fish chips topped list symbols foods associated british .  many pupils also looked english scottish welsh rather british notion european hardly occurred anyone . ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Model implementation** Using Seqtoseq(RNN)\n",
        "\n"
      ],
      "metadata": {
        "id": "eONk1gcSufO7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoder**: The encoder here will outputs a vector and a hidden state and for the next input word uses the hidden state for every input word in the sequence."
      ],
      "metadata": {
        "id": "7-ctuIekuBmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
      ],
      "metadata": {
        "id": "552jFcUSRugj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decoder:**The decoder which will take the encoder output vector and outputs a sequence of the words for creating the translation."
      ],
      "metadata": {
        "id": "Y5Wp6N_fuKel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
      ],
      "metadata": {
        "id": "f5_KyQtARypB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Attention** is used after RNN decoder step before making a prediction."
      ],
      "metadata": {
        "id": "z74iOJYjuXOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 90\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "vVcMQb8TR4Sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)\n",
        "     "
      ],
      "metadata": {
        "id": "R8QewfG6R_8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "metadata": {
        "id": "GMTd-LvQSJOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "metadata": {
        "id": "04_CShQdSMIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "     "
      ],
      "metadata": {
        "id": "18weKS1MSOTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    print(\"Training....\")\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        if iter% 1000 == 0:\n",
        "            print(iter,\"/\",n_iters + 1)\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    return showPlot(plot_losses)\n",
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]\n",
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')\n",
        "     "
      ],
      "metadata": {
        "id": "WtKhkrE5SQol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 100\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "iterations=50\n",
        "trainIters(encoder1, attn_decoder1, iterations, print_every=(iterations//10))\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iF4Pp47mSTgp",
        "outputId": "8df48f90-8059-411e-d4cb-83f2697a0959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training....\n",
            "0m 3s (- 0m 28s) (5 10%) 10.6883\n",
            "0m 5s (- 0m 21s) (10 20%) 12.5121\n",
            "0m 8s (- 0m 19s) (15 30%) 30.3491\n",
            "0m 11s (- 0m 16s) (20 40%) 33.7201\n",
            "0m 12s (- 0m 12s) (25 50%) 40.8651\n",
            "0m 14s (- 0m 9s) (30 60%) 36.0539\n",
            "0m 16s (- 0m 6s) (35 70%) 35.4211\n",
            "0m 17s (- 0m 4s) (40 80%) 30.0166\n",
            "0m 20s (- 0m 2s) (45 90%) 21.2405\n",
            "0m 23s (- 0m 0s) (50 100%) 19.0215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def BLEU(encoder, attention_decoder, n_examples):\n",
        "    total_score = 0\n",
        "    evaluate_pairs = [random.choice(pairs) for i in range(n_examples)]\n",
        "    for pair in evaluate_pairs:\n",
        "        input_sentence = pair[0]\n",
        "        target_words = [pair[1]]\n",
        "        output_words, _ = evaluate(encoder, attention_decoder, input_sentence)\n",
        "        output_words = output_words\n",
        "        score = sentence_bleu(target_words, output_words)\n",
        "        total_score += score\n",
        "    average_BLEU = total_score/len(pairs)\n",
        "    return average_BLEU"
      ],
      "metadata": {
        "id": "IeV2nplmSVjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BLEUScore = BLEU(encoder1, attn_decoder1, 1000)"
      ],
      "metadata": {
        "id": "WXCJK4RoU0jN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d400d7d5-97be-477b-a9e2-e64656d19a12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMCLzfDOU4td",
        "outputId": "727d3258-f651-4061-ed0d-a65c11d162e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> tech\n",
            "= microsoft launches search microsoft unveiled finished version homegrown search engine .  formally launched msn search site takes training wheels test version unveiled november 2003 .  revamped engine indexes pages give direct answers factual questions features tools help people create detailed queries .  microsoft faces challenges establishing serious search site intense competition queries .  google still reigns supreme site people turn often go online answer query keep news search images .  last year google faced greater competition ever users old rivals yahoo microsoft new entrants amazon blinkx try grab searching audience themselves .  renewed interest come realisation many things people online begin search information  particular web page recipe book gadget news story image anything else .  microsoft keen make homegrown search engine significant rival google .  generate corpus data microsoft indexed 5 billion webpages claims update document index every two days  often rivals .  microsoft search engine also answer specific queries directly rather send people page might contain answer .  direct answer feature microsoft calling encarta encyclopaedia provide answers questions definitions facts calculations conversions solutions equations .  tony macklin director product ask jeeves pointed search engine answering specific queries way since april 2003 .  major search providers moved beyond delivering algorithmic search many ways microsoft following market said .  tools sitting alongside msn search engine allow users refine results specific websites countries regions languages .  microsoft also using socalled graphic equalisers let people adjust relevance terms get results uptodate popular .  company said user feedback earlier test versions used refine workings finished system .  test beta version msn search engine unveiled november teething troubles .  first day many new users keen try greeted page said site overwhelmed . \n",
            "< real real real . real \n",
            "\n",
            "> tech\n",
            "= broadband challenges tv viewing number europeans broadband exploded past 12 months web eating tv viewing habits research suggests .  54 million people hooked net via broadband 34 million year ago according market analysts nielsennetratings .  total number people online europe broken 100 million mark .  popularity net meant many turning away tv say analysts jupiter research .  found quarter web users said spent less time watching tv favour net report nielsennetratings found number people fast internet access risen 60 past year .  biggest jump italy rose 120 .  britain close behind broadband users almost doubling year .  growth fuelled lower prices wider choice alwayson fastnet subscription plans .  twelve months ago high speed internet users made one third audience europe 50 expect number keep growing said gabrielle prior nielsennetratings analyst .  number highspeed surfers grows websites need adapt update enhance content retain visitors encourage new ones .  total number europeans online rose 12 100 million past year report showed biggest rise france italy britain germany .  ability browse web pages high speed download files music films play online games changing people spare time .  study analysts jupiter research suggested broadband challenging television viewing habits .  homes broadband 40 said spending less time watching tv .  threat tv greatest countries broadband particular uk france spain said report .  said tv companies faced major longterm threat next five years broadband predicted grow 19 37 households 2009 .  yearonyear continuing see seismic shift europe population consume media information entertainment big implications tv newspaper radio said jupiter research analyst olivier beauvillian . \n",
            "< real real real real . real real . real real real real real real real .  real real real real real . real real real real real real real real . real . real real . real . real real . real real real . real real . real real real . real real real real real real real real real real real real real . real . real real . real real real real real . real real real real real real real . real real real real\n",
            "\n",
            "> tech\n",
            "= playstation 3 chip unveiled details chip designed power sony playstation 3 console released san francisco monday .  sony ibm toshiba working cell processor three years unveil chip technology conference .  chip reported 10 times faster current processors .  designed use graphics workstations new playstation console described supercomputer chip .  sony said cell processor could used bridge gap movies video games .  special effects graphics designed films could ported use directly video game sony told audience e3 exhibition los angeles last year .  cell could also marketed ideal technology televisions supercomputers everything said kevin krewell editor chief microprocessor report .  chip made several different processing cores work tasks together .  playstation 3 expected 2006 developers expecting get prototypes early next year tune games appear launch .  details chip released international solid state circuits conference san francisco .  details already emerged however .  put inside powerful computer servers cell consortium expects capable handling 16 trillion floating point operations calculations every second .  chip also refined able handle detailed graphics common games data demands films broadband media .  ibm said would start producing chip early 2005 manufacturing plants us .  first machines line using cell processor computer workstations servers .  working version ps3 due shown may 2005 full launch next generation console expected start 2006 .  future forms digital content converged fused onto broadband network said ken kutaragi chief operating officer sony said last year .  current pc architecture nearing limits added . \n",
            "< real real . real real real real real real . real real real real real . real real real real real real real real real real real real real real real real real real real real real real real . real real real real real real real real real real real real real . real real real real real . real real real real . real real real real . real real real real real real real real real real . real real real . real real real . real\n",
            "\n",
            "> sport\n",
            "= collins appeals drugs ban sprinter michelle collins lodged appeal eightyear doping ban north american court arbitration sport cas .  33yearold received ban last month result connection federal inquiry balco doping scandal .  first athlete banned without positive drugs test admission drugs use .  cas said ruling normally given within four months appeal .  collins suspended us antidoping agency based patterns observed blood urine tests well evidence balco investigation .  well hit ban collins stripped 2003 world us indoor 200m titles .  san franciscobased balco laboratory centre scandal rocked sport .  company accused distributing illegal performanceenhancing drugs elite athletes . \n",
            "<  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . real .  .  .  .  .  .  .  .  .  .\n",
            "\n",
            "> business\n",
            "= weak dollar trims cadbury profits world biggest confectionery firm cadbury schweppes reported modest rise profits weak dollar took bite results .  underlying pretax profits rose 1 933m 1 . 78bn 2004 would 8 higher currency movements stripped out .  owner brands dairy milk dr pepper snapple generates 80 sales outside uk .  cadbury said confident would hit targets 2005 .  external commercial environment remains competitive confident strategy brands people deliver within goal ranges 2005 said chief executive todd stitzer .  modest profit rise expected analysts company said december poor summer weather hit soft drink sales europe .  cadbury said underlying sales 4 2004 .  growth helped confectionery brands  including cadbury trident halls  enjoyed successful year likeforlike sales 6 .  drinks sales 2 strong growth us carbonated soft drinks led dr pepper diet drinks offset weaker sales europe .  cadbury added fuel growth costcutting programme saved 75m 2004 bringing total cost savings 100m since scheme began mid2003 .  programme set close 20 group factories shed 10 workforce .  cadbury schweppes employs 50 000 people worldwide 7 000 uk . \n",
            "< real .  .  .  .  .  .  .  .  .  . real .  .  .  .  . real .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . real .  .  .  .  .  .  .  .  .  .\n",
            "\n",
            "> entertainment\n",
            "= charges tv cosby us comedian bill cosby face charges stemming allegation sexual misconduct .  authorities philadelphia said found insufficient evidence support woman allegations regarding alleged incident january 2004 .  woman reported allegations canadian authorities last month .  cosby lawyer walter phillips jr said comedian pleased decision .  looks forward moving life said .  district attorney bruce l castor jr charge case said detectives could find instance anyone complained law enforcement conduct would constitute criminal offence .  also said fact woman waited year coming forward contact cosby time also factors decision .  unidentified woman lawyer dolores troiani said client likely sue comedian .  think avenue open her .  felt strong case telling truth .  also said woman supplied evidence prosecutors believed strengthened allegations .  cosby emerged one first black comics mainstream success us .  successful standup hosting children show fat albert cosby kids starring cosby show one biggest sitcoms 1980s . \n",
            "<  .  .  .  .  .  .  .  .  .  .  .  .  .  . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
            "\n",
            "> entertainment\n",
            "= campaigners attack mtv sleaze mtv criticised incessant sleaze television indecency campaigners us .  parents television council ptc monitors violence sex tv said cable music channel offered cheapest form programming .  group forefront vociferous campaign clean american television .  spokeswoman mtv said unfair inaccurate single mtv criticism .  ptc monitored mtv output 171 hours 20 march 27 march 2004 channel spring break coverage .  report  mtv smut peddlers targeting kids sex drugs alcohol  ptc said witnessed 3 056 flashes nudity sexual situations 2 881 verbal references sex .  brent bozell ptc president conservative activist said mtv blatantly selling raunchy sex kids .  compared broadcast television programmes aimed adults mtv programming contains substantially sex foul language violence  mtv shows aimed children young 12 .  question tv influences attitudes perceptions young viewers mtv deliberately marketing raunch millions innocent children .  watchdog decided look mtv programmes janet jackson infamous wardrobe malfunction last year super bowl .  breastbaring incident generated 500 000 complaints cbs  owned parent company mtv  quick apologise .  mtv spokeswoman jeannie kedas said network follows standards broadcasters reflects culture viewers interested in .  unfair inaccurate paint mtv brush irresponsibility said .  think underestimating young people intellect level sophistication .  ms kedas also highlighted fact mtv award 2004 fight rights series focused issues sexual health tolerance . \n",
            "<  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .\n",
            "\n",
            "> business\n",
            "= world leaders gather face uncertainty 2 000 business political leaders around globe arriving swiss mountain resort davos annual world economic forum wef .  five days discuss issues ranging china economic power iraq future sunday elections .  uk prime minister tony blair south african president thabo mbeki among 20 government leaders heads state leaders attending meeting .  unlike previous years protests wef expected muted .  antiglobalisation campaigners called demonstration planned weekend .  brazilian city porto alegre host rival world social forum timed run parallel wef ritzier event davos .  organisers brazilian gathering brings together thousands campaigners globalisation fair trade many causes promised set alternative agenda swiss summit .  however many issues discussed porto alegre davos talking points well .  global warming features particularly high .  wef participants asked offset carbon emissions cause travelling event .  davos deep frost .  snow piled high across mountain village night wind chill takes temperatures minus 20c less .  ultimately forum dominated business issues  outsourcing corporate leadership  bosses fifth world 500 largest companies scheduled attend .  much media focus political leaders coming davos least agenda year forum seems lack overarching theme .  taking responsibility tough choices year official talking point hinting welter knotty problems .  one thing seems sure though transatlantic disagreements deal iran iraq china set dominate discussions .  pointedly one senior official president bush new administration scheduled attend .  us government may still make conciliatory gesture happened year ago vice president dick cheney made surprise appearance davos .  ukraine new president viktor yushchenko speak days inauguration event crowned civil protests rigged first election tried keep power .  european union top leaders among german chancellor gerhard schroeder european commission president manuel barosso too .  mr blair formally open proceedings although speech preempted french president jacques chirac announced attendance last minute secured slot special message two hours mr blair speaks .  organisers also hope new palestinian leader mahmoud abbas use opportunity talks least one three israeli deputy prime ministers coming event list includes shimon peres .  davos fans still hark back 1994 talks yassir arafat mr peres came close peace deal .  mr blair appearance keenly watched political observers uk claim calculated snub political rival chancellor gordon brown supposed lead uk government delegation .  microsoft founder bill gates world richest man regular davos focus campaigning good causes though business interests wholly absent either .  already donated billions dollars fight aids malaria mr gates call world leaders support global vaccination campaign protect children developing countries easily preventable diseases .  tuesday mr gates pledged 750m 400m money support cause .  mr gates company software giant microsoft also hopes use davos shore defences open source software like linux threaten microsoft near monopoly computer desktops .  mr gates said trying arrange meeting brazil president lula da silva .  brazilian government plans switch government computers microsoft linux .  davos global problem solving networking never far apart . \n",
            "< real .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . real .  .  .  .  .  .  .  .  .  .  .  .  .  .\n",
            "\n",
            "> sport\n",
            "= holmes starts 2005 gb events kelly holmes start 2005 series races britain .  holmes make first track appearance home soil since winning double olympic gold january norwich union international glasgow .  also run grand prix birmingham february may defend indoor aaa 800m title sheffield earlier month .  still competitive still want win said .  athlete wait get back track .  added events also great opportunity thank british public enormous levels support given moment stepped plane greece .  glasgow meeting see holmes compete 1500m fiveway match sweden france russia italy . \n",
            "<  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .\n",
            "\n",
            "> tech\n",
            "= putting face big brother literally putting face technology could one keys improving interaction hitech gadgets .  imagine surveillance system also presents virtual embodiment person screen react behaviour perhaps even alert new emails .  basic versions socalled avatars already exist .  together speech voice recognition systems could replace keyboard mouse near future .  ideas showcased london science museum part future face exhibition .  one avatar jeremiah .  virtual man download free install computer .  creator richard bowden lecturer centre vision speech signal processing university surrey refers jeremiah rather it .  jeremiah virtual face attempts emulate humans way responds activity .  childlike likes visual stimulus told bbc news website .  sees children running laughing waving smiles them .  ignore gets angry .  leave gets sad .  also even surprise him .  jeremiah actually intelligent .  works vision reacting preset way information provided surveillance tracker system .  able talk hear least yet .  surrey team already working jeremiah next version replace human face underwater interactive creature finn fish .  interested interaction providing ability system watch going make decisions based explained dr bowden .  research comes time people cope increasing number hitech gadgets .  experts say much natural way interact devices virtual human could make much easier make new gizmos .  get three clock morning go downstairs probably two things going do either going bathroom maybe going make cup tea said dr bowden .  system watch behaviour time learn would predict going turn lights even get kettle could switched on .  might even able tell home surveillance system going away holiday ask could make sure house secure left .  might sound like scary vision orwellian future .  might depend face watching you .  put surveillance cameras centre lot people unhappy fact system watching said dr bowden .  jeremiah camera went nobody minded although still watching could see watching . \n",
            "< real real . real real real real real real real real real real real real real real real real real real real real real real real real . real real . real real real real real real real real real . real real . real real real real real real real real .  real real real real real real real real real . real . real . real real real real real real real real real real real real real real real real real real real real real .\n",
            "\n"
          ]
        }
      ]
    }
  ]
}